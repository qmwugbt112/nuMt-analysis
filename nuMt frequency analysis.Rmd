---
title: "Analysis of nuMt allele frequencies"
author: "Richard Nichols"
date: "21/12/2020"
output: html_document
---
```{css, echo = FALSE}
.bordered {
  border: solid;
}
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Pre-processing in the data


### Initial parameter choices 

```{r, echo=TRUE}

# The lowest average frequency the minor allele 
cutoff <- 0.01
# The no. of SNPs to sample (glmer gets indigestion with the full dataset)
snpNo <- 200 

# mixed effects library with the lmer function
library(lme4)
# knitr utilities
library(knitr)

```


### Raw data

The datafile (read from github.com/qmwugbt112/nuMt-analysis), gives

  * **mapDep**: the proportion of all reads that are mtDNA-like (i.e. are either organellar or nuMt)
  * counts of 
    + **main**: the main allele (i.e. the one shared with organellar DNA sequence)
    + **alt**: alternate allele (i.e. the one restricted to nuMt DNA sequences)
    
  * names for 
    + the **SNP**
    + the **individual** insect sampled
  * **polymorphic**: a boolean variable identifying sites thought to be polymorphic in the organellar mitochondrial DNA (which would muddy the analysis). 

```{r , echo=FALSE, cache=TRUE}
temp <- tempfile()
download.file("https://github.com/qmwugbt112/nuMt-analysis/raw/main/mapDepFreq.csv.zip", temp)
binDat <- read.csv(unz(temp,'mapDepFreq.csv'))
rm(temp)

kable(binDat[1:7,], caption="Raw data")
```



```{r, include=FALSE, cache=TRUE}
# Some initial data processing creates 3 matrixes
# arranged so all the information from one individual is in a row 
# (& each SNP site is a column):
#   altMat: counts of the rarer allele
#   mainMat: counts of the common allele and another of its alternative)
#   polymat: TRUE values to indicate that the site is thought
#             to be polymorphic in live mitochondria
#             (& hence excluded from this analysis)

altMat <- with(binDat, matrix(alt, 
                              nrow = nlevels(individual),
                              ncol = nlevels(SNP)
                              )
               )

mainMat <- with(binDat, matrix(main, 
                               nrow = nlevels(individual),
                               ncol = nlevels(SNP)
                               )
                )

polyMat <- with(binDat, matrix(polymorphic, 
                            nrow = nlevels(individual),
                            ncol = nlevels(SNP))
                )

# Sum the columns of the numerical matixes
altsum <- colSums(altMat)
mainsum <- colSums(mainMat)

# Identify the columns of TRUE values in polyMat 
# (which indicute the SNP sites to be excluded)
polysum <- apply(polyMat,2,unique)

```

### Initial filtering
These three plots show

* The distribution of raw allele frequency data for each SNP across all individuals
* The truncated data (Frequencies > 1%)
* The sub-sampled data (SNPs selected in proportion to their frequency)

Subsampling is required because so the lmer function will run. Higher-frequency alleles are more informative about the proportion of the nuclear genome that is nuMt DNA, so they more heavily sampled. However we wish to cover the whole frequency spectruum; in case rare alleles show different trends, for example.

```{r, echo=FALSE, cache=TRUE}
par(mfrow = c(3,1))
par(mar=c(2,1,1,1))

# investigate shape of the frequency distribution all data
freq <- altsum/(altsum+mainsum)
h <- hist(freq,
       breaks = 0:500 / 500 * 0.4, 
       main = '', 
       xlab = 'Frequency')
text(x = 0.15, 
     y= max(h$counts)*0.6,
     paste('Raw allele frequencies. N=',sum(h$counts),'SNPs')
     )

# repeat for truncated data
h <- hist(freq[(!polysum) & (freq > cutoff)],
          breaks = 0:500 / 500 * 0.4,
          main = '',
          xlab = 'Frequency')
text(x = 0.15,
     y= max(h$counts)*0.6,
     paste('Truncated allele frequencies. N=',sum(h$counts),'SNPs')
     )

# calculate boolean vector to dataframe to subsample SNPs for analysis
# only choose sites without mt polymorphism and relatively frequent
choice1 <- (!polysum) & (freq > cutoff)

# we will be weighting sampling probability towards high freq loci
# and exclude those we have ruled out above by setting their freq2 to zero
freq2 <- freq
freq2[!choice1] <- 0



# Choose 200 loci at random weighting by allele frequency
choice2 <- sample(1:length(freq),
                  size = snpNo,
                  prob = freq2)
h <- hist(freq[choice2],
          breaks = 0:500 / 500 * 0.4,
          main = '',
          xlab = 'Frequency')
text(x = 0.15,
     y= max(h$counts)*0.6,
     paste('Subsampled SNPs. N=',sum(h$counts),'SNPs')
     )

# reset the graphics window
par(mfrow = c(1,1))

```

### Viewing the subsampled data

We expect the frequency of the non-organellar allele to be diluted in samples with more organellar DNA, according to the relationship 
$$p_{obs} = \frac{c}{c+m}p_{n} ;$$ 
where $p_{obs}$ is the observed frequency of the alternate allele, $p_n$ is the true frequency of that allele among the nuclear sequences, $c$ is the unknown relative frequency of nuMts and $c+m$ is the observed relative frequency of mitochondrial-like sequences (frequencies calculated as a proportion of the rest of the nuclear genome). Hence 
$$log(p_{obs}) = -log(c+m) + p_n + c .$$

Consequently a plot of $log(p_{obs})$ aginst $-log(c+m)$ should have a slope of 1 and an intercept of $log(p_n) + log(c)$.  It will vary among alleles, and the SNPs with a higher value of $p_n$ give a lower bound on $c$.

An inital plot shows most loci fit a 1:1 relationships (Lines are the 1:1 plots with the highest and lowest intercept), but with some outliers (indicated).  The older data, sequenced with a different technology, fit a similar relationship, but the observed frequencies are higher, especially for the rarer alleles. Consequently the two datasets were treated separately in the subsequent analysis.

```{r, echo=FALSE, cache=TRUE}
# Create a boolean vector to record T for the selected SNPs F for the others
chosen <- rep(FALSE, length(freq))
chosen[choice2] <- TRUE

# repeat each element by the number of individuals so that the new
# vector can be used to select records from the full dataset.
chosen <- rep(chosen, each = nlevels(binDat$individual))

# Create a dataframe with just the subsampled data
subDat <- subset(binDat, chosen)

# read the factor on top of itself to reduce the levels in SNP to 200
subDat$SNP <- factor(subDat$SNP)

# cleanup - remove all but dataframes and snpNo
listing <- ls()
rm(list = listing[ -c(
  grep("Dat",listing), 
  grep("No",listing))])



# calculate x & y values for the regression
# the -qlogis function converts mapped reads / (all reads)
# to log(unmapped reads / mapped reads)
subDat$xvals <- with(subDat, -qlogis(mapDep))
subDat$yvals <- with(subDat, log(alt/(alt+main)))

# identify the older data using different sequencing technology
subDat$older <- with(subDat, factor(
  is.element(individual, levels(individual)[47:52]))
  )

# remove allele frequencies of zero (-Inf log)
subDat$yvals[subDat$yvals == -Inf] <- NA

# construct a regression through the orgin
# with slope 1 (offset = xvals)
mod1 <- lm(yvals ~ 0 + older + SNP,
           offset = xvals,
           data = subDat)

# store the results needed for plotting
cvals <- coef(mod1)
minval <- min(cvals[-(1:2)] + cvals[1])
maxval <- max(cvals[-(1:2)] + cvals[1])

par(mar = c(4,4,3,1))

# plot the data and regression
with(subDat,
     plot(xvals, yvals,
          main = 'Raw frequency data\n(older data in blue)',
          xlab = '(Unmapped reads / Mapped)',
          ylab = 'Allele Frequency (log scale)',
          col = rainbow(2)[as.numeric(older)],
          cex = 0.2,
          xlim = c(0, max(xvals) * 1.01),
          ylim = c(minval, max(c(0,yvals), na.rm = T) * 1.1),
          xaxt = 'n',
          yaxt = 'n'
          )
     
     )

axis(1, at = log(2^(0:3*3)), labels = 2^(0:3*3)) 
ll <- expression("0", "10"^-1, "10"^-2,"10"^-3,"10"^-4,"10"^-5,"10"^-6)
axis(2, at = log(10^(0:-6)), labels = ll)

text(log(20), log(0.09), "Outliers \U2192")

abline(minval, 1)
abline(maxval, 1)
abline(h = -max(subDat$xvals), lty = 3)
abline(v = 0, lty = 3)


```

### Identifying outlying SNP loci 

The initial inspection suggested that a subset of loci deviate from the typical regression of log$(p_obs)$ vs -log$(c + m)$.  Deviant loci were identified by fitting a mixed effects model with a different slope for each locus:

```{r,  include=FALSE}

# Choose a colour score for each SNP depending on its intercept in the simple regression
# calculate the raw intercepts for recent samples 
cs <- c(cvals[1],cvals[-(1:2)] + cvals[1])

# allocate a colour score according to the rank intercept
SNPcol <- rank(cs, ties.method = 'random')
subDat$colours <- rep(SNPcol, each = nlevels(subDat$individual))

# Separate out the data for the older dataset using a different sequencing technology
oldDat <- subset(subDat, (older == TRUE))
newDat <- subset(subDat, (older == FALSE))

# correct nlevels for individuals in each dataset by reading back in changed factors
oldDat$individual <- factor(oldDat$individual)
newDat$individual <- factor(newDat$individual)

```


```{r, cache=TRUE,echo=FALSE, class.output = ".bordered"}
# find rogue SNPs which have an outlying slope in the regression for the larger recent dataset



mod2 <- lmer(yvals ~ SNP + xvals + xvals:SNP + (1 | individual), 
             data = newDat)

longSummary <- summary(mod2)
print(mod2@call)

```

The outliers were selected by inspection of a histogram of slopes for each SNP.

```{r, echo=FALSE}

# find the SNP:xval interaction terms in mod1
iterms <- grep('xvals',names(longSummary$coefficients[,1]))
slopes <- longSummary$coefficients[iterms,1]

# Convert interaction terms from differences to raw slope values
# by adding the first value to all subsequent values
slopes[-1] <- slopes[-1] + slopes[1]

# spot outlying SNPs (markedly different slope from the rest)
hist(slopes, breaks = 40)
abline(v = 0.7, col = 'red')

rogueSNPs <- which(abs(slopes)<0.7)
text(0.2,15,"Outlying slopes", col = "red")



# Put the classification of rogue SNPs into the dataframes

# convert the SNP index numbers to SNP names
rogueSNPs <- with(newDat, levels(SNP)[rogueSNPs])

# apply these classifications and identify rouges in the two dataframes
newDat$rogueSNP <- with(newDat, is.element(SNP, rogueSNPs))
oldDat$rogueSNP <- with(oldDat, is.element(SNP, rogueSNPs))

```

### The regression patterns in the rogue SNPs

The rogue SNPs do not follow the expected 1:1 relationship with -log$(c+m)$ = log(unmapped reads/mapped)

```{r, echo=FALSE, cache=TRUE}

with(subset(newDat,rogueSNP),
     plot(xvals, yvals,
          main = 'Rogue SNPs',
          xlab = '(Unmapped reads / Mapped)',
          ylab = 'Allele Frequency (log scale)',
          col = rainbow(snpNo)[colours],
          cex = 0.2,
          xlim = c(0, max(xvals) * 1.01),
          ylim = c(minval, max(c(0,yvals), na.rm = T) * 1.1),
          xaxt = 'n',
          yaxt = 'n'
     )
)

axis(1, at = log(2^(0:3*3)), labels = 2^(0:3*3)) 
ll <- expression("0", "10"^-1, "10"^-2,"10"^-3,"10"^-4,"10"^-5,"10"^-6)
axis(2, at = log(10^(0:-6)), labels = ll)


```
## Estimating the proportion of nuMt DNA in the nucleus
The remaining data can be fitted to a model of a 1:1 slope by using $(c+m)$ as an offset.  Individual is treated as a random effect. There are two bounds on c: an upper bound estimated from the smallest relative frequency of (c+m) and a lower bound estimated from the highest intercept. 
The intercept is log($c$) +log($p_n$) so the lower estimate will will be close to $c$ for SNPs with high frequency of the nuMt allele.

```{r, echo=FALSE, class.output = ".bordered"}

# Create a dataset with the well behaved SNPs
goodnewDat <- subset(newDat, !rogueSNP)
# Read back in the SNP factor to get levels right
goodnewDat$SNP <- with(goodnewDat, factor(SNP))

# Fit the 1:1 modle for each SNP
mod3 <- lmer(yvals ~ 0 + SNP + (1 | individual),
             offset = xvals,
             data = goodnewDat)

# extract intercepts
cvals <- summary(mod3)$coefficients[,1]

# choose a colour for each SNP according to its intercept
scols <- rank(cvals,
              ties.method = 'random')
goodnewDat$colours <- rep(scols,
                          each = nlevels(goodnewDat$individual))



with(goodnewDat,
     plot(xvals, yvals,
          main = 'Estimating nuMt Proportions',
          xlab = '(Unmapped reads / Mapped)',
          ylab = 'Allele Frequency (log scale)',
          col = rainbow(snpNo)[colours],
          cex = 0.2,
          xlim = c(0, max(xvals) * 1.01),
          ylim = c(min(cvals), max(c(0,yvals), na.rm = T) * 1.1),
          xaxt = 'n',
          yaxt = 'n'
      
     )
)

axis(1, at = log(2^(0:3*3)), labels = 2^(0:3*3)) 
ll <- expression("0", "10"^-1, "10"^-2,"10"^-3,"10"^-4,"10"^-5,"10"^-6)
axis(2, at = log(10^(0:-6)), labels = ll)

for (c in 1:length(cvals)) abline(cvals[c], 1, col = rainbow(snpNo)[scols[c]], lwd = 0.05)
abline(v = 0, lty = 3)
abline(h = - max(goodnewDat$xvals), lty = 3)

cHi <- format(100*plogis(-max(goodnewDat$xvals)), digits =2)
cat("Upper estimate of c  (the lowest of (c+m)):   ",
      cHi,"%")

cLo <- format(100*exp(max(cvals)), digits =2)
cat("Lower estimate of c  (the highest intercept):   ",
      cLo,"%")
```


### analysis of residual variation

The residual variation is an estimate of the variation of allele frequencies among nuMts, which might have a geographical structure (nearby populations might be expected to have similar frequencies).  The residual pattern of residual variation among samples was therefore used to construct a Principle components analysis


```{r, echo=FALSE, class.output = ".bordered"}





# use a glm allowing us to deal with zero allele frequencies
mod4 <- glm(cbind(alt,main) ~ 0 + SNP,
            offsett = xvals,
            family = binomial(link = "log"),
            data = goodnewDat)

# store the raw residuals (i.e. on the natural scale) of mod3
goodnewDat$resid<- residuals(mod4, type = "response")
hist(residuals(mod4, type = "response"))

# create a individual by SNP matrix of residuals
resMat <- with (goodnewDat, matrix(resid,
                                   nrow = nlevels(individual),
                                   ncol = nlevels(SNP)))

# find the mean for each row (individual)
iMeans <- rowMeans(resMat)

resMat2 <- resMat - iMeans


# identify the values included in the mod3 regression
# i.e. had a non-zero observed frequency
include <- !(is.na(goodnewDat$yvals))



# carry out the pca analysis
pca <- prcomp(resMat2, scale. = TRUE)

# extract the names for each individual
iNames <- with(goodnewDat,
               as.character(individual[1:nlevels(individual)]))
# get the length of each name
ln <- nchar(iNames)

# extract the last letter of the name
ll <- substr(iNames, ln, ln)

# extract the other characters (should be a number)
oc <- substr(iNames, 1, (ln-1))

# convert the last letter of the name (F,T or U) to (1,2 or 3)
cols <- as.numeric(factor(ll))


plot(pca$x[,1:2], col = rainbow(3)[cols])

plot(pca$x[cols == 2,1:2], type = 'n')
for (i in 1:length(cols)) if (cols[i]==2) {
  text(oc[i],x = pca$x[i,1], y= pca$x[i,2])
  }

```

........................................................................
This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.
